{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mintusf/CIFAR/blob/master/1110.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ6EkaBxOiJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "b04c79aa-f1cb-42cf-92fa-f9c5db856817"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmPjrMHgOkoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSLK8MsxOl33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "1c2b5ad0-ff73-497d-b588-f8ffd4faf392"
      },
      "source": [
        "link = 'https://drive.google.com/open?id=19b0kUvuPGl6U4Cj4VgTfYTc7gK5VgA09'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Train_X')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1ejj9Qt4Ah9IOSmKnM87-oGe-IytAxM2F'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Train_Y')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1KzllioQ321D7zAHlmasN-tagGcE6pCnC'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Dev_X')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1m3E519AgJkPbsTNGvEwSHCN6otXNB1Y0'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Dev_Y')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=10mFBsqasbIcrbLrWqqkMyw41OczlcYID'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Test_X')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1r3zB1bwH6ZjrLZjQftaqHAr-9wGra3Uj'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Test_Y')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1B3YZrKYnxvAQmC-I6i8vm998qP5izhbA'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Dev_X_original')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1mxKxn8s1qMCoX9mvBAd-0wCUKQUSVZ2p'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Dev_Y_original')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1z2TFC3cFLsN7qQcg86ILb2Gv4lXUM19m'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Test_X_original')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1TL3GJFCvwxwK5DqqZhTveb_jKmPS0AiK'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Test_Y_original')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1Fa_uK2z1fpL6GyRosGQvtFx5eYjz7IdJ'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Train_X_augmented')\n",
        "\n",
        "link = 'https://drive.google.com/open?id=170pj5QRqB4RlxakUaZZ5PXjD1cAu_ItV'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Train_Y_augmented')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19b0kUvuPGl6U4Cj4VgTfYTc7gK5VgA09\n",
            "1ejj9Qt4Ah9IOSmKnM87-oGe-IytAxM2F\n",
            "1KzllioQ321D7zAHlmasN-tagGcE6pCnC\n",
            "1m3E519AgJkPbsTNGvEwSHCN6otXNB1Y0\n",
            "10mFBsqasbIcrbLrWqqkMyw41OczlcYID\n",
            "1r3zB1bwH6ZjrLZjQftaqHAr-9wGra3Uj\n",
            "1B3YZrKYnxvAQmC-I6i8vm998qP5izhbA\n",
            "1mxKxn8s1qMCoX9mvBAd-0wCUKQUSVZ2p\n",
            "1z2TFC3cFLsN7qQcg86ILb2Gv4lXUM19m\n",
            "1TL3GJFCvwxwK5DqqZhTveb_jKmPS0AiK\n",
            "1Fa_uK2z1fpL6GyRosGQvtFx5eYjz7IdJ\n",
            "170pj5QRqB4RlxakUaZZ5PXjD1cAu_ItV\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK8uouIrOvAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.load('Train_X')/255.\n",
        "Y_train_1d = np.load('Train_Y')\n",
        "X_dev = np.load('Dev_X')/255.\n",
        "Y_dev_1d = np.load('Dev_Y')\n",
        "X_test = np.load('Test_X')/255.\n",
        "Y_test_1d = np.load('Test_Y')\n",
        "X_dev_original = np.load('Dev_X_original')/255.\n",
        "Y_dev_original_1d = np.load('Dev_Y_original')\n",
        "X_test_original = np.load('Test_X_original')/255.\n",
        "Y_test_original_1d = np.load('Test_Y_original')\n",
        "X_train_augmented = np.load('Train_X_augmented')/255.\n",
        "Y_train_augmented_1d = np.load('Train_Y_augmented')\n",
        "(X_train_original, Y_train_original), _ = cifar10.load_data()\n",
        "X_train_original = X_train_original/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RfHCm78OxOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = ['airplane',\n",
        " 'automobile',\n",
        " 'bird',\n",
        " 'cat',\n",
        " 'deer',\n",
        " 'dog',\n",
        " 'frog',\n",
        " 'horse',\n",
        " 'ship',\n",
        " 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0jahUNPOzjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "308cfd40-d023-4a0f-d877-2a542a5f0559"
      },
      "source": [
        "Y_train = np.eye(10)[Y_train_1d.reshape(-1)]\n",
        "print(Y_train.shape)\n",
        "Y_dev = np.eye(10)[Y_dev_1d.reshape(-1)]\n",
        "print(Y_dev.shape)\n",
        "Y_dev_original = np.eye(10)[Y_dev_original_1d.reshape(-1)]\n",
        "print(Y_dev_original.shape)\n",
        "Y_train_augmented = np.eye(10)[Y_train_augmented_1d.reshape(-1)]\n",
        "print(Y_train_augmented.shape)\n",
        "Y_train_original = np.eye(10)[Y_train_original.reshape(-1)]\n",
        "print(Y_train_original.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42500, 10)\n",
            "(8750, 10)\n",
            "(5000, 10)\n",
            "(50000, 10)\n",
            "(50000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMDN5x1cDO37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Autoencoder (input_img):\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    #x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "    x = UpSampling2D((2,2))(encoded)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = UpSampling2D((2, 2))(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    return (encoded, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_h6BZPKO2A5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d878f353-da91-468c-a83e-997103222038"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Nov  6 23:19:54 2019\n",
        "\n",
        "@author: Filip\n",
        "\"\"\"\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  # -*- coding: utf-8 -*-\n",
        "  \"\"\"\n",
        "  Created on Tue Nov  5 22:21:34 2019\n",
        "\n",
        "  @author: Filip\n",
        "  \"\"\"\n",
        "\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from math import ceil\n",
        "  from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout,UpSampling2D\n",
        "  from tensorflow.keras.models import Model, load_model\n",
        "\n",
        "  from tensorflow.keras.initializers import glorot_uniform\n",
        "  from tensorflow.keras.utils import plot_model\n",
        "\n",
        "  def random_mini_batches(X, Y, minibatch_size=64):\n",
        "      m = X.shape[0]\n",
        "      X_minibatches = []\n",
        "      Y_minibatches = []\n",
        "\n",
        "    # Shuffling X and Y\n",
        "\n",
        "      permutation = list(np.random.permutation(m))\n",
        "      shuffled_X = X[permutation, :, :, :]\n",
        "      shuffled_Y = Y[permutation, :]\n",
        "\n",
        "    # Dividing into batches\n",
        "\n",
        "      minibatches_no = ceil(m / minibatch_size)\n",
        "      for k in range(0, minibatches_no):\n",
        "          mini_batch_X = shuffled_X[k * minibatch_size:k\n",
        "                  * minibatch_size + minibatch_size, :, :, :]\n",
        "          mini_batch_Y = shuffled_Y[k * minibatch_size:k\n",
        "                  * minibatch_size + minibatch_size, :]\n",
        "          X_minibatches.append(mini_batch_X)\n",
        "          Y_minibatches.append(mini_batch_Y)\n",
        "\n",
        "      return (X_minibatches, Y_minibatches)\n",
        "\n",
        "\n",
        "  input_img = Input(shape = (32,32,3))\n",
        "  model_autoenc = Model(input_img,Autoencoder(input_img)[1])\n",
        "\n",
        "\n",
        "  # As seen in the Keras Tutorial Notebook, prior training a model, you need to configure the learning process by compiling the model.\n",
        "\n",
        "  # In[12]:\n",
        "  Adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  model_autoenc.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "  # Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. \n",
        "\n",
        "  # In[14]:\n",
        "  model_autoenc.summary()\n",
        "  history = model_autoenc.fit(X_train, X_train, validation_data=(X_dev_original, X_dev_original), epochs = 20, batch_size = 128)\n",
        "\n",
        "\n",
        "  # In[15]:\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "  idx = np.random.randint(50000,size = 25)\n",
        "\n",
        "\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  \"\"\"\n",
        "  for i in idx:\n",
        "      step1 = data[i,:].reshape(3,32,32)\n",
        "      step2 = np.transpose(step1,axes = (1,2,0))\n",
        "      plt.imshow(step2)\n",
        "      plt.show()\n",
        "      print(names[labels[i]])\n",
        "  \"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 3)         1731      \n",
            "=================================================================\n",
            "Total params: 670,851\n",
            "Trainable params: 669,315\n",
            "Non-trainable params: 1,536\n",
            "_________________________________________________________________\n",
            "Train on 42500 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "42500/42500 [==============================] - 56s 1ms/sample - loss: 0.0060 - acc: 0.7215 - val_loss: 0.0303 - val_acc: 0.6217\n",
            "Epoch 2/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0025 - acc: 0.7851 - val_loss: 0.0025 - val_acc: 0.7861\n",
            "Epoch 3/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0019 - acc: 0.8033 - val_loss: 0.0024 - val_acc: 0.8065\n",
            "Epoch 4/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0016 - acc: 0.8117 - val_loss: 0.0019 - val_acc: 0.7586\n",
            "Epoch 5/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0014 - acc: 0.8167 - val_loss: 0.0021 - val_acc: 0.8146\n",
            "Epoch 6/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0014 - acc: 0.8199 - val_loss: 0.0017 - val_acc: 0.7913\n",
            "Epoch 7/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0012 - acc: 0.8259 - val_loss: 0.0016 - val_acc: 0.7992\n",
            "Epoch 8/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0012 - acc: 0.8311 - val_loss: 0.0016 - val_acc: 0.7781\n",
            "Epoch 9/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0011 - acc: 0.8352 - val_loss: 0.0015 - val_acc: 0.8511\n",
            "Epoch 10/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0012 - acc: 0.8289 - val_loss: 0.0018 - val_acc: 0.8023\n",
            "Epoch 11/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0011 - acc: 0.8311 - val_loss: 0.0016 - val_acc: 0.8040\n",
            "Epoch 12/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 9.7666e-04 - acc: 0.8383 - val_loss: 9.0132e-04 - val_acc: 0.8092\n",
            "Epoch 13/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 8.9129e-04 - acc: 0.8416 - val_loss: 0.0011 - val_acc: 0.8079\n",
            "Epoch 14/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 8.9445e-04 - acc: 0.8442 - val_loss: 0.0014 - val_acc: 0.8050\n",
            "Epoch 15/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 9.0584e-04 - acc: 0.8426 - val_loss: 0.0012 - val_acc: 0.7926\n",
            "Epoch 16/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 9.0622e-04 - acc: 0.8437 - val_loss: 0.0351 - val_acc: 0.7002\n",
            "Epoch 17/20\n",
            "42500/42500 [==============================] - 53s 1ms/sample - loss: 0.0011 - acc: 0.8332 - val_loss: 0.0031 - val_acc: 0.7666\n",
            "Epoch 18/20\n",
            "33920/42500 [======================>.......] - ETA: 10s - loss: 8.9091e-04 - acc: 0.8421"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-3mJxg9F_BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_autoenc.save_weights('autoencoder.h5')\n",
        "model_autoenc.get_weights()[0][1][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mzEJ8ghPfai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "with tf.device('/device:GPU:0'):\n",
        "  \n",
        "  from tensorflow.keras.datasets import cifar10\n",
        "  from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "  from tensorflow.keras import optimizers\n",
        "  import numpy as np\n",
        "  #from tensorflow.keras.layers.core import Lambda\n",
        "  from tensorflow.keras import backend as K\n",
        "  from tensorflow.keras import regularizers\n",
        "\n",
        "  class cifar10vgg:\n",
        "      def __init__(self,learning_rate,x_train,y_train,train=True):\n",
        "          self.num_classes = 10\n",
        "          self.weight_decay = 0.0005\n",
        "          self.x_shape = [32,32,3]\n",
        "          encoded = Autoencoder(input_img)[0] #using encoder\n",
        "          self.model = Model(input_img,self.build_model(encoded))\n",
        "          for l1,l2 in zip(self.model.layers[:8],model_autoenc.layers[:8]):#getting weights from encoder\n",
        "            print(\"1\")\n",
        "            l1.set_weights(l2.get_weights())\n",
        "          #print(model_autoenc.get_weights()[0][1][1])\n",
        "          #print(self.model.get_weights()[0][1][1])\n",
        "          for layer in self.model.layers[0:8]:\n",
        "            layer.trainable = False\n",
        "          if train:\n",
        "              self.model.summary()\n",
        "              self.model = self.train(self.model,learning_rate,x_train,y_train)\n",
        "          else:\n",
        "              self.model.load_weights('cifar10vgg.h5')\n",
        "\n",
        "\n",
        "      def build_model(self,encoded):\n",
        "          # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
        "\n",
        "          weight_decay = self.weight_decay\n",
        "\n",
        "          X = Conv2D(64, (3, 3), padding='same',\n",
        "                          input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay))(encoded)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          #X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "          X = Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)  \n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          #X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "          X = Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)      \n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)    \n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)     \n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)     \n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Flatten()(X)\n",
        "          X = Dense(512,kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)  \n",
        "          X = Dropout(0.5)(X)\n",
        "\n",
        "          X = Dense(self.num_classes)(X)\n",
        "          X = Activation('softmax')(X)\n",
        "          \n",
        "          return X\n",
        "\n",
        "\n",
        "      def normalize(self,X_train,X_test):\n",
        "          #this function normalize inputs for zero mean and unit variance\n",
        "          # it is used when training a model.\n",
        "          # Input: training set and test set\n",
        "          # Output: normalized training set and test set according to the trianing set statistics.\n",
        "          mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "          std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "          X_train = (X_train-mean)/(std+1e-7)\n",
        "          X_test = (X_test-mean)/(std+1e-7)\n",
        "          return X_train, X_test\n",
        "\n",
        "      def normalize_production(self,x):\n",
        "          #this function is used to normalize instances in production according to saved training set statistics\n",
        "          # Input: X - a training set\n",
        "          # Output X - a normalized training set according to normalization constants.\n",
        "\n",
        "          #these values produced during first training and are general for the standard cifar10 training set normalization\n",
        "          mean = 120.707\n",
        "          std = 64.15\n",
        "          return (x-mean)/(std+1e-7)\n",
        "          \n",
        "\n",
        "      def predict(self,x,normalize=True,batch_size=50):\n",
        "          if normalize:\n",
        "              x = self.normalize_production(x)\n",
        "          return self.model.predict(x,batch_size)\n",
        "\n",
        "      def train(self,model_classifier,learning_rate,x_train,y_train):\n",
        "\n",
        "          #training parameters\n",
        "          batch_size = 128\n",
        "          maxepoches = 200\n",
        "          learning_rate = learning_rate\n",
        "          lr_decay = 1e-5\n",
        "          lr_drop = 20\n",
        "          # The data, shuffled and split between train and test sets:\n",
        "          #(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "          #x_train = x_train.astype('float32')\n",
        "          #x_test = x_test.astype('float32')\n",
        "          #x_train, x_test = self.normalize(x_train, x_test)\n",
        "\n",
        "          #y_train = tf.keras.utils.to_categorical(y_train, self.num_classes)\n",
        "          #y_test = tf.keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "          def lr_scheduler(epoch):\n",
        "              return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "          reduce_lr = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "          #data augmentation\n",
        "          datagen = ImageDataGenerator(\n",
        "              featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "              samplewise_center=False,  # set each sample mean to 0\n",
        "              featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "              samplewise_std_normalization=False,  # divide each input by its std\n",
        "              zca_whitening=False,  # apply ZCA whitening\n",
        "              rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "              width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "              height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "              zoom_range=0.1,\n",
        "              shear_range=0.1,\n",
        "              horizontal_flip=True,  # randomly flip images\n",
        "              vertical_flip=False)  # randomly flip images\n",
        "          # (std, mean, and principal components if ZCA whitening is applied).\n",
        "          datagen.fit(x_train)\n",
        "\n",
        "\n",
        "\n",
        "          #optimization details\n",
        "          sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "\n",
        "          mask = [1, 1, 2, 2, 2, 1, 1, 1, 1, 1]\n",
        "          def customLoss(yTrue,yPred):\n",
        "            return K.categorical_crossentropy(mask*yTrue, mask*yPred, from_logits=False, axis=-1)\n",
        "          #model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "          model_classifier.compile(loss=customLoss, optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "          # training process in a for loop with learning rate drop every 25 epoches.\n",
        "\n",
        "          history = model_classifier.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                          batch_size=batch_size),\n",
        "                              steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                              epochs=maxepoches,\n",
        "                              validation_data=(x_test, y_test),verbose=1)\n",
        "          model_classifier.save_weights('cifar10vgg.h5')\n",
        "          \n",
        "          plt.plot(history.history['acc'])\n",
        "          plt.plot(history.history['val_acc'])\n",
        "          plt.title('Model accuracy')\n",
        "          plt.ylabel('Accuracy')\n",
        "          plt.xlabel('Epoch')\n",
        "          plt.legend(['Train', 'Test'], loc='upper left')\n",
        "          plt.show()\n",
        "\n",
        "          # Plot training & validation loss values\n",
        "          plt.plot(history.history['loss'])\n",
        "          plt.plot(history.history['val_loss'])\n",
        "          plt.title('Model loss')\n",
        "          plt.ylabel('Loss')\n",
        "          plt.xlabel('Epoch')\n",
        "          plt.legend(['Train', 'Test'], loc='upper left')\n",
        "          plt.show()\n",
        "\n",
        "          return model_classifier\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xFQOL1lLZTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = X_train_augmented\n",
        "x_test = X_dev_original\n",
        "y_train = Y_train_augmented\n",
        "y_test = Y_dev_original\n",
        "x_train = X_train\n",
        "y_train = Y_train\n",
        "\n",
        "model_classifier1 = cifar10vgg(0.001,x_train,y_train)\n",
        "model_classifier2 = cifar10vgg(0.005,x_train,y_train)\n",
        "model_classifier3 = cifar10vgg(0.01,x_train,y_train)\n",
        "\n",
        "#predicted_x = model_classifier.predict(x_test)\n",
        "#residuals = np.argmax(predicted_x,1)!=np.argmax(y_test,1)\n",
        "\n",
        "#loss = sum(residuals)/len(residuals)\n",
        "#print(\"the validation 0/1 loss is: \",loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciWjL_4IsdWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Autoencoder (input_img):\n",
        "\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    #x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "    x = UpSampling2D((2,2))(encoded)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = UpSampling2D((2, 2))(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    return (encoded, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ldw1OMEukcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_autoenc = Model(input_img,Autoencoder(input_img)[1])\n",
        "\n",
        "Adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model_autoenc.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSoNklnhuzXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_classifier4 = cifar10vgg(0.001,x_train,y_train)\n",
        "model_classifier5 = cifar10vgg(0.005,x_train,y_train)\n",
        "model_classifier6 = cifar10vgg(0.01,x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r_0_i-Ty4x6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "with tf.device('/device:GPU:0'):\n",
        "  \n",
        "  from tensorflow.keras.datasets import cifar10\n",
        "  from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "  from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "  from tensorflow.keras import optimizers\n",
        "  import numpy as np\n",
        "  #from tensorflow.keras.layers.core import Lambda\n",
        "  from tensorflow.keras import backend as K\n",
        "  from tensorflow.keras import regularizers\n",
        "\n",
        "  class cifar10vgg:\n",
        "      def __init__(self,learning_rate,x_train,y_train,train=True):\n",
        "          self.num_classes = 10\n",
        "          self.weight_decay = 0.0005\n",
        "          self.x_shape = [32,32,3]\n",
        "          encoded = Autoencoder(input_img)[0] #using encoder\n",
        "          self.model = Model(input_img,self.build_model(encoded))\n",
        "          for l1,l2 in zip(self.model.layers[:8],model_autoenc.layers[:8]):#getting weights from encoder\n",
        "            print(\"1\")\n",
        "            l1.set_weights(l2.get_weights())\n",
        "          #print(model_autoenc.get_weights()[0][1][1])\n",
        "          #print(self.model.get_weights()[0][1][1])\n",
        "          for layer in self.model.layers[0:8]:\n",
        "            layer.trainable = False\n",
        "          if train:\n",
        "              self.model.summary()\n",
        "              self.model = self.train(self.model,learning_rate,x_train,y_train)\n",
        "          else:\n",
        "              self.model.load_weights('cifar10vgg.h5')\n",
        "\n",
        "\n",
        "      def build_model(self,encoded):\n",
        "          # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
        "\n",
        "          weight_decay = self.weight_decay\n",
        "\n",
        "          X = Conv2D(64, (3, 3), padding='same',\n",
        "                          input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay))(encoded)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.2)(X)\n",
        "\n",
        "          X = Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.2)(X)\n",
        "\n",
        "          #X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "          X = Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)  \n",
        "          X = Dropout(0.2)(X)\n",
        "\n",
        "          X = Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.2)(X)\n",
        "\n",
        "          #X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "          X = Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)      \n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)    \n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = MaxPooling2D(pool_size=(2, 2))(X)\n",
        "\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)     \n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)     \n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)\n",
        "          X = Dropout(0.3)(X)\n",
        "\n",
        "          X = Flatten()(X)\n",
        "          X = Dense(512,kernel_regularizer=regularizers.l2(weight_decay))(X)\n",
        "          X = BatchNormalization()(X)\n",
        "          X = Activation('relu')(X)  \n",
        "          X = Dropout(0.4)(X)\n",
        "\n",
        "          X = Dense(self.num_classes)(X)\n",
        "          X = Activation('softmax')(X)\n",
        "          \n",
        "          return X\n",
        "\n",
        "\n",
        "      def normalize(self,X_train,X_test):\n",
        "          #this function normalize inputs for zero mean and unit variance\n",
        "          # it is used when training a model.\n",
        "          # Input: training set and test set\n",
        "          # Output: normalized training set and test set according to the trianing set statistics.\n",
        "          mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "          std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "          X_train = (X_train-mean)/(std+1e-7)\n",
        "          X_test = (X_test-mean)/(std+1e-7)\n",
        "          return X_train, X_test\n",
        "\n",
        "      def normalize_production(self,x):\n",
        "          #this function is used to normalize instances in production according to saved training set statistics\n",
        "          # Input: X - a training set\n",
        "          # Output X - a normalized training set according to normalization constants.\n",
        "\n",
        "          #these values produced during first training and are general for the standard cifar10 training set normalization\n",
        "          mean = 120.707\n",
        "          std = 64.15\n",
        "          return (x-mean)/(std+1e-7)\n",
        "          \n",
        "\n",
        "      def predict(self,x,normalize=True,batch_size=50):\n",
        "          if normalize:\n",
        "              x = self.normalize_production(x)\n",
        "          return self.model.predict(x,batch_size)\n",
        "\n",
        "      def train(self,model_classifier,learning_rate,x_train,y_train):\n",
        "\n",
        "          #training parameters\n",
        "          batch_size = 128\n",
        "          maxepoches = 200\n",
        "          learning_rate = learning_rate\n",
        "          lr_decay = 1e-5\n",
        "          lr_drop = 20\n",
        "          # The data, shuffled and split between train and test sets:\n",
        "          #(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "          #x_train = x_train.astype('float32')\n",
        "          #x_test = x_test.astype('float32')\n",
        "          #x_train, x_test = self.normalize(x_train, x_test)\n",
        "\n",
        "          #y_train = tf.keras.utils.to_categorical(y_train, self.num_classes)\n",
        "          #y_test = tf.keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "          def lr_scheduler(epoch):\n",
        "              return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "          reduce_lr = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "          #data augmentation\n",
        "          datagen = ImageDataGenerator(\n",
        "              featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "              samplewise_center=False,  # set each sample mean to 0\n",
        "              featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "              samplewise_std_normalization=False,  # divide each input by its std\n",
        "              zca_whitening=False,  # apply ZCA whitening\n",
        "              rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "              width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "              height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "              zoom_range=0.1,\n",
        "              shear_range=0.1,\n",
        "              horizontal_flip=True,  # randomly flip images\n",
        "              vertical_flip=False)  # randomly flip images\n",
        "          # (std, mean, and principal components if ZCA whitening is applied).\n",
        "          datagen.fit(x_train)\n",
        "\n",
        "\n",
        "\n",
        "          #optimization details\n",
        "          sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "\n",
        "          mask = [1, 1, 2, 2, 2, 1, 1, 1, 1, 1]\n",
        "          def customLoss(yTrue,yPred):\n",
        "            return K.categorical_crossentropy(mask*yTrue, yPred, from_logits=False, axis=-1)\n",
        "          #model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "          model_classifier.compile(loss=customLoss, optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "          # training process in a for loop with learning rate drop every 25 epoches.\n",
        "\n",
        "          history = model_classifier.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                          batch_size=batch_size),\n",
        "                              steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                              epochs=maxepoches,\n",
        "                              validation_data=(x_test, y_test),verbose=1)\n",
        "          model_classifier.save_weights('cifar10vgg.h5')\n",
        "          \n",
        "          plt.plot(history.history['acc'])\n",
        "          plt.plot(history.history['val_acc'])\n",
        "          plt.title('Model accuracy')\n",
        "          plt.ylabel('Accuracy')\n",
        "          plt.xlabel('Epoch')\n",
        "          plt.legend(['Train', 'Test'], loc='upper left')\n",
        "          plt.show()\n",
        "\n",
        "          # Plot training & validation loss values\n",
        "          plt.plot(history.history['loss'])\n",
        "          plt.plot(history.history['val_loss'])\n",
        "          plt.title('Model loss')\n",
        "          plt.ylabel('Loss')\n",
        "          plt.xlabel('Epoch')\n",
        "          plt.legend(['Train', 'Test'], loc='upper left')\n",
        "          plt.show()\n",
        "\n",
        "          return model_classifier\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlTmc57U3iWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Autoencoder (input_img):\n",
        "\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    #x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "    x = UpSampling2D((2,2))(encoded)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = UpSampling2D((2, 2))(x)\n",
        "    #x = Conv2D(3, (3, 3), activation='relu', padding='same')(x)\n",
        "    #x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    return (encoded, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVUmiy8t3lq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_autoenc = Model(input_img,Autoencoder(input_img)[1])\n",
        "\n",
        "Adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model_autoenc.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23DSqhLb3pFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_classifier7 = cifar10vgg(0.001,x_train,y_train)\n",
        "model_classifier8 = cifar10vgg(0.005,x_train,y_train)\n",
        "model_classifier9 = cifar10vgg(0.01,x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgr2yL1S3vKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}